# -*- coding: utf-8 -*-
"""script_Sandeep_Sohal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nbOA0xmT3Gg4db6AXZrvU-hnX_-vXpx0
"""

import sys
 
 
print("User Current Version:-", sys.version)

#!python Assignment1 (1).py

from sklearn import datasets 
import matplotlib.pyplot as plt
from sklearn import tree
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, GridSearchCV

############## FOR EVERYONE ##############
# Please note that the blanks are here to guide you for this first assignment, but the blanks are  
# in no way representative of the number of commands/ parameters or length of what should be inputted.

### PART 1 ###
# Scikit-Learn provides many popular datasets. The breast cancer wisconsin dataset is one of them. 
# Write code that fetches the breast cancer wisconsin dataset. 
# Hint: https://scikit-learn.org/stable/datasets/toy_dataset.html
# Hint: Make sure the data features and associated target class are returned instead of a "Bunch object".

#1. fetch the wisconsin dataset
from sklearn.datasets import load_breast_cancer
breast_cancer_data=datasets.load_breast_cancer()
X,y = datasets.load_breast_cancer(return_X_y=True, as_frame=True) #(5 points) 

X.head()

type(y)
X.shape
y.shape

# Check how many instances we have in the dataset, and how many features describe these instances
print("There are", len(X.axes[0]) , "instances described by", len(X.axes[1]) , "features.") #(5 points)

# Create a training and test set such that the test set has 40% of the instances from the 
# complete breast cancer wisconsin dataset and that the training set has the remaining 60% of  
# the instances from the complete breast cancer wisconsin dataset, using the holdout method. 
# In addition, ensure that the training and test sets 
# contain approximately the same 
# percentage of instances of each target class as the complete set.
 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, stratify=y, random_state = 42)  #(5 points) 

print(X_train.shape)

# Create a decision tree classifier. Then Train the classifier using the training dataset created earlier.
# To measure the quality of a split, using the entropy criteria.
# Ensure that nodes with less than 6 training instances are not further split
clf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=6)  #(5 points) 
clf = clf.fit(X_train, y_train)  #(4 points)

# Apply the decision tree to classify the data 'testData'.
predC = clf.predict(X_test)  #(4 points) 

# Compute the accuracy of the classifier on 'testData'
print('The accuracy of the classifier is', accuracy_score(y_test, predC))  #(3 points)

# Visualize the tree created. Set the font size the 12 (5 points) 

fig = plt.figure(figsize=(30,30))
Dtree = tree.plot_tree(clf, 
                   feature_names=breast_cancer_data.feature_names,  
                   class_names=breast_cancer_data.target_names,
                   filled=True, fontsize=12)

### PART 2.1 ###
# Visualize the training and test error as a function of the maximum depth of the decision tree
# Initialize 2 empty lists where you will save the training and testing accuracies 
# as we iterate through the different decision tree depth options.
trainAccuracy = []  #(1 point) 
testAccuracy = [] #(1 point) 
# Use the range function to create different depths options, ranging from 1 to 15, for the decision trees
depthOptions = list(range(1,15)) #(1 point) 
for depth in depthOptions: #(1 point) 
    # Use a decision tree classifier that still measures the quality of a split using the entropy criteria.
    # Also, ensure that nodes with less than 6 training instances are not further split
    cltree = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=6) #(1 point) 
    # Decision tree training
    cltree = cltree.fit(X_train, y_train) #(1 point) 
    # Training error
    y_predTrain = cltree.predict(X_train) #(1 point) 
    # Testing error
    y_predTest = cltree.predict(X_test) #(1 points) 
    # Training accuracy
    trainAccuracy.append(accuracy_score(y_train,y_predTrain)) #(2 points) 
    # Testing accuracy
    testAccuracy.append(accuracy_score( y_test,y_predTest)) #(2 points)

# Plot of training and test accuracies vs the tree depths (use different markers of different colors)
#ax.plot(______,______,______,______,______,______) #(3 points) 
#______.______(['Training Accuracy','Test Accuracy']) # add a legend for the training accuracy and test accuracy (1 point) 
#______.______('Tree Depth') # name the horizontal axis 'Tree Depth' (1 point) 
#______.______('Classifier Accuracy') # name the horizontal axis 'Classifier Accuracy' (1 point) 

import numpy as np

depth = np.arange(len(depthOptions)) + 1
plt.plot(depth, trainAccuracy, label='Training Accuracy') 
plt.plot(depth, testAccuracy, label='Testing Accuracy') 
plt.xlabel('Tree Depth') # name the horizontal axis 'Tree Depth' (1 point) 
plt.ylabel('Classifier Accuracy') # name the horizontal axis 'Classifier Accuracy' (1 point) 
plt.legend()  # add a legend for the training accuracy and test accuracy (1 point) 
plt.show() # Show graph



# Fill out the following blanks: #(6 points (3 points per blank)) 
""" 
According to the test error, the best model to select is when the maximum depth is equal to 9, approximately. 
But, we should not use select the hyperparameters of our model using the test data, because this is considered overfitting. 
This is because if we tune our hyperparameters based on our test data then we will have a model that can perform well on the test data, but we are not sure if this model will still perform well given a new instance that it has not been trained on. 
"""

### PART 2.2 ###
# Use sklearn's GridSearchCV function to perform an exhaustive search to find the best tree depth.
# Hint: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
# First define the parameter to be optimized: the max depth of the tree
parameters = {"max_depth":[1,2,3,4,5,6,7,None]} #(6 points)
# We will still grow a decision tree classifier by measuring the quality of a split using the entropy criteria. 
# Also, continue to ensure that nodes with less than 6 training instances are not further split.

clf=GridSearchCV(tree.DecisionTreeClassifier(criterion='entropy',min_samples_split=6), parameters, verbose=1, cv=10 )  #(6 points)
clf=clf.fit(X_train, y_train) #(5 points)
tree_model = clf.best_estimator_ #(5 points)
y_test_pred=clf.predict(X_test)

print(tree_model)
print(clf.best_score_)
print("The maximum depth of the tree should be", clf.best_params_) #(5 points)

#import libraries
import sklearn
import pydotplus, graphviz
from sklearn.tree import export_graphviz

# The best model is tree_model. Visualize that decision tree (tree_model). Set the font size the 12 
plt.figure(figsize=(30,30))
treevisual=tree.plot_tree(tree_model, feature_names=breast_cancer_data.feature_names, class_names=breast_cancer_data.target_names, filled=True, fontsize=12 ) #(5 points)

# Fill out the following blank: #(3 points)
""" 
This method for tuning the hyperparameters of our model is acceptable, because it will increase the performance of the model and give users the best value of hyperparameters. 
Hyperparameters are set by the user building the model and methods such as GridSearchCV along with cross-validation can be used to find the optimal hyperparameters.
GridSearchCV is a hyperparameter tuning technique that extracts the best model paramters from a given grid of parameters [1].
"""

# Explain below was is tenfold Stratified cross-validation?  #(5 points)
"""
K-fold cross validation technique is used for creating machine learning models that are robust [2]. 
It essentially divides the training data that the model will train and test on. The k parameter is usually selected by the builder of the model and it is usually 5 or 10. 
In this assignment the cv is given the value 10. This will divide the training data into 10 folds. For example, the dataset used in this assignment has 341 training samples and if k=10 then each fold will contain 34 training instances (341/10=34.1).
The entire training process will contain 10 experiments, each experiment will take the first 34 instances which will be used for testing and the rest for training. The algorithm will do this 10 times going over each fold.
The model performance is computed after each experiment and the final accuracy will be computed by taking the mean of all the accuracies computed.   
However in cases where our dataset is not balanced then the k-fold cross validation will also not produce the best results. In such cases the stratified k-fold cross validation technique is used. 
The stratified cross-validation technique will distribute the data uniformly across each fold. In all the 10 folds there will be samples belonging to each class and this will ensure that the model learns and trains in the most optimal way as possible. 
If the k-fold cross validation is not stratified it might contain samples belonging to only one class in one fold and this will result in a faulty model which is why the stratified cross validation technique is prefered. 
"""

############## FOR GRADUATE STUDENTS ONLY (the students enrolled in CPS 8318) ##############
### PART 3: decision tree classifier to solve a classification problem ###
""" 
Please note that the grade for parts 1 and 2 counts for 75% of your total grade. The following
work requires you to work on a project of your own and will account for the remaining 25% of your grade.

Choose a practical dataset (as opposed to the example ones we used in class) with a reasonable size 
from one of the following sources (other sources are also possible, e.g., Kaggle):

•	UCI Machine Learning Repository, https://archive.ics.uci.edu/ml/datasets.php. 

•	KDD Cup challenges, http://www.kdd.org/kdd-cup.

Download the data, read the description, and use a decision tree classifier approach to solve a 
classification problem as best as you can. Write up a report of approximately 1 page, double spaced, 
in which you briefly describe the dataset (e.g., the size – number of instances and number of attributes, 
what type of data, source), the problem, the approaches that you tried and the results. 
You can use any appropriate libraries. 

Your tasks are:
1.	research how to pre-process data on your own, if needed by the dataset you chose.
2.	to report on which attributes are most important for your classifier 
(hint: the feature that gives you the most information gain about the class labels).
3.	to report on anything else inventive you can think to do, but the above 2 tasks would be enough. 

Marking: Part 3 accounts for 25% of your final grade. 50% of that will be for the writeup and 50% 
for the results. In the write-up, cite the sources of your data and ideas, and use your own words 
to express your thoughts. If you have to use someone else's words or close to them, use quotes and 
a citation.  The citation is a number in brackets (like [1]) that refers to a similar number in the 
references section at the end of your paper or in a footnote, where the source is given as an author, 
title, URL or journal/conference/book reference. Grammar is important. Concerning the 50% for results, 
elaborate on what (if any) manipulations you did, what are the results for the algorithms you tried, 
what else you tried. 

Submit the python script (.py file(s)) with your redacted document (PDF file) on the D2L site. 
If the dataset is not in the public domain, you also need to submit the data file. 
Name your documents appropriately:
report_Firstname_LastName.pdf
script_ Firstname_LastName.py
"""

#import all libraries

import numpy as np
import pandas as pd
from sklearn import datasets 
import matplotlib.pyplot as plt

from sklearn import tree
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, GridSearchCV

import seaborn as sns

# import dataset
#!unzip '/content/archive (19).zip'

data= pd.read_csv('/content/Churn_Modelling.csv')
df=pd.DataFrame(data)

df.head()

#exploratory data analysis and checking for any missing values
df.info()

#drop samples with missing values
df.isnull().sum()

df.shape

plt.figure(figsize=(20,20))
df.hist('Exited')
plt.show('Exited')

df['Exited'].value_counts() #we have a highly unbalanced dataset

"""AFTER EDITING THE DATASET"""

#select 2037 samples belonging to each class 

data= pd.read_csv('/content/churndata.csv')
df=pd.DataFrame(data)

df.head()

#exploratory data analysis and checking for any missing values
df.info()

#drop samples with missing values
df.isnull().sum()

df.shape

plt.figure(figsize=(20,20))
df.hist('Exited')
plt.show('Exited')

df['Exited'].value_counts() #now we have a balanced dataset

sns.displot(df, x="Geography")

#we can drop attributes that are not relevant
drop_features=['RowNumber', 'CustomerId', 'Surname', 'Geography', 'Gender']

df= df.drop(drop_features, axis=1)
df.head()

df.shape

#get dummy variables for one hot encoding
#df=pd.get_dummies(df, columns=['Geography', 'Gender'])

#now we can shuffle the dataset
from sklearn.utils import shuffle

df=shuffle(df, random_state=34)

df.head()

#split data

X= df.drop('Exited', axis=1)
y=df['Exited']

#X.shape
y.shape

#train test split
 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=25)

#build the decision tree classifier
clf = tree.DecisionTreeClassifier(criterion = 'entropy')
clf = clf.fit(X_train, y_train)

# Apply the decision tree to classify the test data
y_test_pred = clf.predict(X_test)

print("The accuracy of the classifier is", accuracy_score(y_test, y_test_pred))

df.head()

from IPython.display import Image
from six import StringIO
from sklearn.tree import export_graphviz
import pydotplus, graphviz

def print_tree (clf):
  dot_data= StringIO()
  
  export_graphviz (clf, out_file=dot_data, filled=True, rounded=True,
                   feature_names=X.columns,
                   class_names= ['Exited', 'NotExited'])
  graph=pydotplus.graph_from_dot_data(dot_data.getvalue())
  return Image(graph.create_png())

print_tree(clf)

importance= clf.feature_importances_

print(importance)

for i,v in enumerate (importance):
  print('Feature: %0d, Score: %.5f' % (i,v))



"""References for Part 2:

[1] paranavkotak, “DASKGRIDSEARCHCV - a competitor for GRIDSEARCHCV,” GeeksforGeeks, 11-Aug-2021. Available: https://www.geeksforgeeks.org/daskgridsearchcv-a-competitor-for-gridsearchcv/#:~:text=GridSearchCV%20is%20a%20technique%20to,then%20the%20predictions%20are%20made. 

[2] V. Lendave, “Hands-on tutorial on performance measure of stratified k-fold cross-validation,” Analytics India Magazine, 20-Nov-2021. Available: https://analyticsindiamag.com/hands-on-tutorial-on-performance-measure-of-stratified-k-fold-cross-validation/. 


"""

